{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import parse\n",
    "from nltk.grammar import CFG\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_LINE Jamaica-bound trains skip 75 Av Briarwood and Sutphin Blvd\n"
     ]
    }
   ],
   "source": [
    "note = \"F_LINE D_LINE Manhattan-bound trains skip 25 St and Union St\"\n",
    "note1 =  'F_LINE Jamaica-bound trains skip 75 Av, Briarwood and Sutphin Blvd'\n",
    "note1 = note1.replace(',', '')\n",
    "tok = word_tokenize(note)\n",
    "print(note1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('Sent', [Tree('S', [Tree('Sub', ['F_LINE']), Tree('Direction', [Tree('bound', ['Jamaica-bound']), Tree('trains', ['trains'])])]), Tree('VP', [Tree('Status', ['skip']), Tree('Stations', [Tree('Stop', [Tree('num', ['75']), Tree('Street', ['Av'])]), Tree('Stop', [Tree('Loc', ['Briarwood'])]), Tree('and', ['and']), Tree('Stop', [Tree('Loc', ['Sutphin']), Tree('Street', ['Blvd'])])])])])]\n",
      "(Sent\n",
      "  (S (Sub F_LINE) (Direction (bound Jamaica-bound) (trains trains)))\n",
      "  (VP\n",
      "    (Status skip)\n",
      "    (Stations\n",
      "      (Stop (num 75) (Street Av))\n",
      "      (Stop (Loc Briarwood))\n",
      "      (and and)\n",
      "      (Stop (Loc Sutphin) (Street Blvd)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = CFG.fromstring(\"\"\"\n",
    "Sent -> S VP\n",
    "S -> Sub | Sub Direction | Sub Sub Direction\n",
    "Direction -> bound trains\n",
    "VP -> Status Stations\n",
    "Stations -> Stop | Stop and Stop | Stop Stop and Stop\n",
    "Stop -> num Street | Loc Street | Loc\n",
    "\n",
    "\n",
    "Sub -> \"F_LINE\" | \"D_LINE\"\n",
    "bound -> \"Manhattan-bound\" | \"Jamaica-bound\"\n",
    "trains -> \"trains\"\n",
    "Status -> \"skip\"\n",
    "num -> \"25\" | \"75\"\n",
    "Street -> \"St\" | \"Av\" | \"Blvd\"\n",
    "Loc -> \"Union\" | \"Sutphin\" | \"Briarwood\"\n",
    "Service -> \"service\"\n",
    "and -> \"and\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.RecursiveDescentParser(grammar1)\n",
    "tree2 = parser.parse(note1.split(' '))\n",
    "a = []\n",
    "for t in tree2:\n",
    "    a.append(t)\n",
    "    print(a)\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "for t in tree2:\n",
    "    b.append(t)\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = {'sdf': ['1','2']}\n",
    "dic['sdf'].append('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sub', 'F_LINE']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(a[0][0][0]).replace('(', '').replace(')', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Chart.parses at 0x1188855c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.parse import generate\n",
    "from nltk.grammar import Nonterminal\n",
    "\n",
    "# Above generate generates ['which', 'sri lankan', 'player', 'has', 'the', 'most', 'runs']\n",
    "# But the same sentence is not parsable by ChartParser.\n",
    "\n",
    "result1 = nltk.ChartParser(grammar1).parse(note.split(' '))\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cp = nltk.RegexpParser('''\n",
    "    NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "    P: {<IN>}           # Preposition\n",
    "    V: {<V.*>}          # Verb\n",
    "    PP: {<P> <NP>}      # PP -> P NP\n",
    "    VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "chunk structures must contain tagged tokens or trees",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0922473bb244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                 \u001b[0mchunk_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, chunk_struct, trace)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0mchunkstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChunkString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;31m# Apply the sequence of rules to the chunkstring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chunk_struct, debug_level)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'><'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_struct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'><'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/chunk/regexp.py\u001b[0m in \u001b[0;36m_tag\u001b[0;34m(self, tok)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             raise ValueError('chunk structures must contain tagged '\n\u001b[0m\u001b[1;32m    108\u001b[0m                              'tokens or trees')\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: chunk structures must contain tagged tokens or trees"
     ]
    }
   ],
   "source": [
    "result = cp.parse()\n",
    "print(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'park'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-4189ca6387f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtree1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/parse/recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Harrison/anaconda/lib/python3.6/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 648\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'park'\"."
     ]
    }
   ],
   "source": [
    "from nltk import parse\n",
    "from nltk.grammar import CFG\n",
    "\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | Det N PP\n",
    "VP -> V \n",
    "PP -> P NP\n",
    "NP -> 'I'\n",
    "N -> 'man'  | 'telescope' | 'dog'\n",
    "Det -> 'the' | 'a'\n",
    "P -> 'in' | 'with'\n",
    "V -> 'saw'\n",
    "\"\"\")\n",
    "\n",
    "sent = 'I saw a man in the park'\n",
    "tok = word_tokenize(sent)\n",
    "\n",
    "parser = nltk.RecursiveDescentParser(grammar)\n",
    "tree1 = parser.parse(tok)\n",
    "for t in tree1:\n",
    "    print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
